{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FcVmoIs2kJm"
      },
      "source": [
        "This is the gym (from https://github.com/aylint/gym-minesweeper).\n",
        "Made some changes to get it working properly with smaller board sizes, also:\n",
        "- 0,0 is always the first action and never a mine\n",
        "- a random action will never be an uncovered cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6moCPD62guE"
      },
      "source": [
        "import sys\n",
        "from six import StringIO\n",
        "from random import randint\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "# cell values, non-negatives indicate number of neighboring mines\n",
        "MINE = -1\n",
        "CLOSED = 9\n",
        "\n",
        "def stringify(board):\n",
        "  # int(bool()) instead of string; convert to tertiary\n",
        "  # print(''.join([str(board[x][y]) for x in range(len(board)) for y in range(len(board))]))\n",
        "  return ''.join([str(board[x][y]) if board[x][y] == 0 or board[x][y] == CLOSED else '1' for x in range(len(board)) for y in range(len(board))])\n",
        "\n",
        "\n",
        "def is_new_move(my_board, x, y):\n",
        "    return my_board[x, y] == CLOSED\n",
        "\n",
        "def is_valid(size, x, y):\n",
        "    return (x >= 0) & (x < size) & (y >= 0) & (y < size)\n",
        "\n",
        "\n",
        "def is_win(my_board, num_mines):\n",
        "    return np.count_nonzero(my_board == CLOSED) == 3\n",
        "\n",
        "\n",
        "def is_mine(board, x, y):\n",
        "    return board[x, y] == MINE\n",
        "\n",
        "\n",
        "def place_mines(board_size, num_mines):\n",
        "    mines_placed = 0\n",
        "    board = np.zeros((board_size, board_size), dtype=int)\n",
        "    while mines_placed < num_mines:\n",
        "        rnd = randint(0, board_size * board_size)\n",
        "        x = int(rnd / board_size)\n",
        "        y = int(rnd % board_size)\n",
        "        if is_valid(board_size, x, y) and not (x == 0 and y == 0):\n",
        "            if not is_mine(board, x, y):\n",
        "                board[x, y] = MINE\n",
        "                mines_placed += 1\n",
        "    return board\n",
        "\n",
        "\n",
        "class MinesweeperDiscreetEnv(gym.Env):\n",
        "    metadata = {\"render.modes\": [\"ansi\", \"human\"]}\n",
        "\n",
        "    def __init__(self, board_size, num_mines):\n",
        "        self.board_size = board_size\n",
        "        self.num_mines = num_mines\n",
        "        self.board = place_mines(board_size, num_mines)\n",
        "        self.my_board = np.ones((board_size, board_size), dtype=int) * CLOSED\n",
        "        self.num_actions = 0\n",
        "\n",
        "#       -2 here? \n",
        "        self.observation_space = spaces.Box(low=-2, high=9,\n",
        "                                            shape=(self.board_size, self.board_size), dtype=np.int)\n",
        "        self.action_space = spaces.Discrete(self.board_size*self.board_size)\n",
        "        self.valid_actions = np.ones((self.board_size * self.board_size), dtype=np.bool)\n",
        "\n",
        "    def count_neighbour_mines(self, x, y):\n",
        "        neighbour_mines = 0\n",
        "        for _x in range(x - 1, x + 2):\n",
        "            for _y in range(y - 1, y + 2):\n",
        "                if is_valid(self.board_size, _x, _y):\n",
        "                    if is_mine(self.board, _x, _y):\n",
        "                        neighbour_mines += 1\n",
        "        return neighbour_mines\n",
        "\n",
        "    def open_neighbour_cells(self, my_board, x, y):\n",
        "        for _x in range(x-1, x+2):\n",
        "            for _y in range(y-1, y+2):\n",
        "                if is_valid(self.board_size, _x, _y):\n",
        "                    if is_new_move(my_board, _x, _y):\n",
        "                        my_board[_x, _y] = self.count_neighbour_mines(_x, _y)\n",
        "                        if my_board[_x, _y] == 0:\n",
        "                            my_board = self.open_neighbour_cells(my_board, _x, _y)\n",
        "        return my_board\n",
        "\n",
        "    def get_next_state(self, state, x, y):\n",
        "        my_board = state\n",
        "        game_over = False\n",
        "        if is_mine(self.board, x, y):\n",
        "            my_board[x, y] = MINE\n",
        "            game_over = True\n",
        "        else:\n",
        "            my_board[x, y] = self.count_neighbour_mines(x, y)\n",
        "            if my_board[x, y] == 0:\n",
        "                my_board = self.open_neighbour_cells(my_board, x, y)\n",
        "        self.my_board = my_board\n",
        "        return my_board, game_over\n",
        "\n",
        "    def randomAction(self, state):\n",
        "      action = 0\n",
        "      while state[int(action / self.board_size)][action % self.board_size] != CLOSED:\n",
        "        action = self.action_space.sample()\n",
        "      return action\n",
        "\n",
        "    def reset(self):\n",
        "        self.my_board = np.ones((self.board_size, self.board_size), dtype=int) * CLOSED\n",
        "        self.board = place_mines(self.board_size, self.num_mines)\n",
        "        self.num_actions = 0\n",
        "        self.valid_actions = np.ones((self.board_size * self.board_size), dtype=bool)\n",
        "\n",
        "        return self.my_board\n",
        "\n",
        "    def step(self, action):\n",
        "        state = self.my_board\n",
        "        x = int(action / self.board_size)\n",
        "        y = int(action % self.board_size)\n",
        "\n",
        "        next_state, reward, done, info = self.next_step(state, x, y)\n",
        "        self.my_board = next_state\n",
        "        self.num_actions += 1\n",
        "        self.valid_actions = (next_state.flatten() == CLOSED)\n",
        "        info['valid_actions'] = self.valid_actions\n",
        "        info['num_actions'] = self.num_actions\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def next_step(self, state, x, y):\n",
        "        my_board = state\n",
        "        if not is_new_move(my_board, x, y):\n",
        "            # print(\"repeat\", my_board, x, y)\n",
        "            return my_board, -0.2, False, {}\n",
        "        while True:\n",
        "            state, game_over = self.get_next_state(my_board, x, y)\n",
        "            if not game_over:\n",
        "                if is_win(state, self.num_mines):\n",
        "                    return state, 1, True, {}\n",
        "                else:\n",
        "                    return state, 0.2, False, {}\n",
        "            else:\n",
        "                return state, -1, True, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
        "        s = stringify(self.my_board)\n",
        "        outfile.write(s)\n",
        "        if mode != 'human':\n",
        "            return outfile"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iLtSSYEta88"
      },
      "source": [
        "For board size 4x4, 3 mines, there are (4^2)(3+2)^(4^2) = 2.441e12 possible state-action pairs in the q-table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08DAbu5F28Jd",
        "outputId": "764898ee-af18-4c4e-e483-ceb60a2c64dd"
      },
      "source": [
        "import random\n",
        "import timeit\n",
        "\n",
        "BOARD_SIZE = 4\n",
        "NUM_MINES = 3\n",
        "\n",
        "start_t = timeit.default_timer()\n",
        "\n",
        "env = MinesweeperDiscreetEnv(board_size=BOARD_SIZE, num_mines=NUM_MINES)\n",
        "\n",
        "total_episodes = 1000000000   # Total episodes\n",
        "learning_rate = 0.7           # Learning rate\n",
        "gamma = 0.1                   # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability \n",
        "decay_rate = 10e-8            # Exponential decay rate for exploration prob\n",
        "\n",
        "qtable = {}\n",
        "\n",
        "# List of rewards\n",
        "rewards = []\n",
        "total_steps = 0\n",
        "win, loss = 0, 0\n",
        "score = 0 #tracking completion\n",
        "\n",
        "print('10000\\tcumul. reward\\tavg. steps\\t|qtable|\\twin rate\\tepsilon\\t\\tavg completion')\n",
        "\n",
        "for episode in range(10000*10):\n",
        "    \n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    state_str = stringify(state)\n",
        "\n",
        "    # Is this state seen? If not, add it to qtable and initialize the action array to 0\n",
        "    if not state_str in qtable: \n",
        "        qtable[state_str] = np.zeros(BOARD_SIZE * BOARD_SIZE)\n",
        "    \n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "    # loop until game over\n",
        "    while(True):\n",
        "        total_steps += 1\n",
        "        state_str = stringify(state)\n",
        "\n",
        "        action = np.argmax(qtable[state_str]) if random.uniform(0,1) > epsilon else env.randomAction(state)\n",
        "\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        new_state_str = stringify(new_state)\n",
        "\n",
        "        total_rewards += reward\n",
        "        state = new_state\n",
        "                \n",
        "        if reward == 1: win +=1\n",
        "        if reward == -1: loss += 1\n",
        "\n",
        "        if not new_state_str in qtable: \n",
        "          qtable[new_state_str] = np.zeros(BOARD_SIZE * BOARD_SIZE)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]   \n",
        "        qtable[state_str][action] += learning_rate * (reward + gamma * np.max(qtable[new_state_str]) - qtable[state_str][action])\n",
        "\n",
        "        if done: \n",
        "          score += np.count_nonzero((state != 9) & (state != -1)) / (BOARD_SIZE * BOARD_SIZE - NUM_MINES)\n",
        "          break\n",
        "\n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
        "    rewards.append(total_rewards)\n",
        "\n",
        "    if (episode % 10000 == 0):\n",
        "      print('{:}\\t{:8.6f}\\t{:8.6f}\\t{:8}\\t{:8.6f}\\t{:8.6f}\\t{:8.6f}'.format(\n",
        "        int(episode/10000),\n",
        "        sum(rewards)/(episode+1),\n",
        "        (total_steps/(episode+1)),\n",
        "        sum(map(lambda k: len(k), qtable.items())),\n",
        "        win/(episode+1),\n",
        "        epsilon,\n",
        "        score/(episode+1)\n",
        "      ))\n",
        "\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "print(\"time\" ,str(stop-start_t))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\tcumul. reward\tavg. steps\t|qtable|\twin rate\tepsilon\t\tavg completion\n",
            "0\t-0.800000\t2.000000\t       6\t0.000000\t1.000000\t0.615385\n",
            "1\t-0.338746\t3.714129\t   12770\t0.059294\t0.999010\t0.581973\n",
            "2\t-0.335833\t3.729864\t   18616\t0.059247\t0.998022\t0.583905\n",
            "3\t-0.327729\t3.754642\t   23288\t0.060798\t0.997034\t0.587014\n",
            "4\t-0.326852\t3.754956\t   26734\t0.061198\t0.996048\t0.587151\n",
            "5\t-0.329557\t3.743385\t   29478\t0.060999\t0.995062\t0.585613\n",
            "6\t-0.329995\t3.749004\t   31900\t0.060249\t0.994078\t0.585149\n",
            "7\t-0.329955\t3.750546\t   34060\t0.060128\t0.993094\t0.585394\n",
            "8\t-0.329956\t3.749903\t   35872\t0.060224\t0.992112\t0.585661\n",
            "9\t-0.330090\t3.749003\t   37442\t0.060266\t0.991130\t0.585806\n",
            "time 82.38725410599994\n"
          ]
        }
      ]
    }
  ]
}
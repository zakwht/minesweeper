{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lmXOOGyTDTIh"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from six import StringIO\n",
        "from random import randint\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "# cell values, non-negatives indicate number of neighboring mines\n",
        "MINE = [-1]\n",
        "CLOSED = [9]\n",
        "\n",
        "def stringify(board):\n",
        "  # int(bool()) instead of string; convert to tertiary\n",
        "  # print(''.join([str(board[x][y]) for x in range(len(board)) for y in range(len(board))]))\n",
        "  return ''.join([str(board[x][y]) if board[x][y] == 0 or board[x][y] == CLOSED else '1' for x in range(len(board)) for y in range(len(board))])\n",
        "\n",
        "\n",
        "def is_new_move(my_board, x, y):\n",
        "    return my_board[x][y] == CLOSED\n",
        "\n",
        "def is_valid(size, x, y):\n",
        "    return (x >= 0) & (x < size) & (y >= 0) & (y < size)\n",
        "\n",
        "\n",
        "def is_win(my_board, num_mines):\n",
        "    return np.count_nonzero(my_board == CLOSED) == num_mines\n",
        "\n",
        "\n",
        "def is_mine(board, x, y):\n",
        "    return board[x, y] == MINE\n",
        "\n",
        "\n",
        "def place_mines(board_size, num_mines):\n",
        "    mines_placed = 0\n",
        "    board = np.zeros((board_size, board_size), dtype=object)\n",
        "    while mines_placed < num_mines:\n",
        "        rnd = randint(0, board_size * board_size)\n",
        "        x = int(rnd / board_size)\n",
        "        y = int(rnd % board_size)\n",
        "        if is_valid(board_size, x, y) and not (x == 0 and y == 0):\n",
        "            if not is_mine(board, x, y):\n",
        "                board[x][y] = MINE\n",
        "                mines_placed += 1\n",
        "    return board\n",
        "\n",
        "\n",
        "class MinesweeperDiscreetEnv(gym.Env):\n",
        "    metadata = {\"render.modes\": [\"ansi\", \"human\"]}\n",
        "\n",
        "    def __init__(self, board_size, num_mines):\n",
        "        self.board_size = board_size\n",
        "        self.num_mines = num_mines\n",
        "        self.board = place_mines(board_size, num_mines)\n",
        "        self.bs = (self.board_size, self.board_size, 1)\n",
        "        # self.board2 = [[x] for x in y for y in board]\n",
        "        self.my_board = np.ones((board_size, board_size, 1), dtype=int) * CLOSED\n",
        "        # print(self.my_board)\n",
        "        # exit()\n",
        "        self.num_actions = 0\n",
        "\n",
        "#       -2 here? \n",
        "        self.observation_space = spaces.Box(low=-2, high=9,\n",
        "                                            shape=(self.board_size, self.board_size), dtype=np.int)\n",
        "        self.action_space = spaces.Discrete(self.board_size*self.board_size)\n",
        "        self.valid_actions = np.ones((self.board_size * self.board_size), dtype=np.bool)\n",
        "\n",
        "    def count_neighbour_mines(self, x, y):\n",
        "        neighbour_mines = 0\n",
        "        for _x in range(x - 1, x + 2):\n",
        "            for _y in range(y - 1, y + 2):\n",
        "                if is_valid(self.board_size, _x, _y):\n",
        "                    if is_mine(self.board, _x, _y):\n",
        "                        neighbour_mines += 1\n",
        "        return neighbour_mines\n",
        "\n",
        "    def open_neighbour_cells(self, my_board, x, y):\n",
        "        for _x in range(x-1, x+2):\n",
        "            for _y in range(y-1, y+2):\n",
        "                if is_valid(self.board_size, _x, _y):\n",
        "                    if is_new_move(my_board, _x, _y):\n",
        "                        my_board[_x][_y] = self.count_neighbour_mines(_x, _y)\n",
        "                        if my_board[_x][_y] == 0:\n",
        "                            my_board = self.open_neighbour_cells(my_board, _x, _y)\n",
        "        return my_board\n",
        "\n",
        "    def get_next_state(self, state, x, y):\n",
        "        my_board = state\n",
        "        game_over = False\n",
        "        if is_mine(self.board, x, y):\n",
        "            my_board[x][y] = MINE\n",
        "            game_over = True\n",
        "        else:\n",
        "            my_board[x][y] = self.count_neighbour_mines(x, y)\n",
        "            if my_board[x][y] == [0]:\n",
        "                my_board = self.open_neighbour_cells(my_board, x, y)\n",
        "        self.my_board = my_board\n",
        "        return my_board, game_over\n",
        "\n",
        "    def randomAction(self, state):\n",
        "      action = 0\n",
        "      while state[int(action / self.board_size)][action % self.board_size] != CLOSED:\n",
        "        # print('trying random', state[int(action / self.board_size)][action % self.board_size], CLOSED)\n",
        "        action = self.action_space.sample()\n",
        "      return action\n",
        "\n",
        "    def reset(self):\n",
        "        self.my_board = np.ones((self.board_size, self.board_size, 1), dtype=int) * CLOSED\n",
        "        self.board = place_mines(self.board_size, self.num_mines)\n",
        "        self.bs = (self.board_size, self.board_size, 1)\n",
        "        # self.board2 = [[x] for x in y for y in board]\n",
        "        self.num_actions = 0\n",
        "        self.valid_actions = np.ones((self.board_size * self.board_size), dtype=bool)\n",
        "\n",
        "        return self.my_board\n",
        "\n",
        "    def step(self, action):\n",
        "        state = self.my_board\n",
        "        x = int(action / self.board_size)\n",
        "        y = int(action % self.board_size)\n",
        "\n",
        "        next_state, reward, done, info = self.next_step(state, x, y)\n",
        "        self.my_board = next_state\n",
        "        self.num_actions += 1\n",
        "        self.valid_actions = (next_state.flatten() == CLOSED)\n",
        "        info['valid_actions'] = self.valid_actions\n",
        "        info['num_actions'] = self.num_actions\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def next_step(self, state, x, y):\n",
        "        my_board = state\n",
        "        if not is_new_move(my_board, x, y):\n",
        "            # print(\"repeat\", my_board, x, y)\n",
        "            return my_board, -0.2, False, {}\n",
        "        while True:\n",
        "            state, game_over = self.get_next_state(my_board, x, y)\n",
        "            if not game_over:\n",
        "                if is_win(state, self.num_mines):\n",
        "                    return state, 1, True, {}\n",
        "                else:\n",
        "                    return state, 0.2, False, {}\n",
        "            else:\n",
        "                return state, -1, True, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
        "        s = stringify(self.my_board)\n",
        "        outfile.write(s)\n",
        "        if mode != 'human':\n",
        "            return outfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LPWXVI1zDlQ0"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Dense, Flatten\n",
        "from keras.optimizers import adam_v2\n",
        "\n",
        "def create_dqn(learn_rate, input_dims, n_actions, conv_units, dense_units):\n",
        "    model = Sequential([\n",
        "                Conv2D(conv_units, (3,3), activation='relu', padding='same', input_shape=input_dims),\n",
        "                Conv2D(conv_units, (3,3), activation='relu', padding='same'),\n",
        "                Conv2D(conv_units, (3,3), activation='relu', padding='same'),\n",
        "                Conv2D(conv_units, (3,3), activation='relu', padding='same'),\n",
        "                Flatten(),\n",
        "                Dense(dense_units, activation='relu'),\n",
        "                Dense(dense_units, activation='relu'),\n",
        "                Dense(n_actions, activation='linear')])\n",
        "\n",
        "    model.compile(optimizer=adam_v2.Adam(lr=learn_rate, epsilon=1e-4), loss='mse')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5JyNTxHQDuHW"
      },
      "outputs": [],
      "source": [
        "import os, sys, random\n",
        "\n",
        "ROOT = os.getcwd()\n",
        "sys.path.insert(1, f'{os.path.dirname(ROOT)}')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "# Environment settings\n",
        "MEM_SIZE = 50_000 # number of moves to store in replay buffer\n",
        "MEM_SIZE_MIN = 1_000 # min number of moves in replay buffer\n",
        "\n",
        "# Learning settings\n",
        "BATCH_SIZE = 64\n",
        "learn_rate = 0.01\n",
        "LEARN_DECAY = 0.99975\n",
        "LEARN_MIN = 0.001\n",
        "DISCOUNT = 0.1 #gamma\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 0.95\n",
        "EPSILON_DECAY = 0.999975\n",
        "EPSILON_MIN = 0.01\n",
        "\n",
        "# DQN settings\n",
        "CONV_UNITS = 64 # number of neurons in each conv layer\n",
        "DENSE_UNITS = 512 # number of neurons in fully connected dense layer\n",
        "UPDATE_TARGET_EVERY = 5\n",
        "\n",
        "# Default model name\n",
        "MODEL_NAME = f'conv{CONV_UNITS}x4_dense{DENSE_UNITS}x2_y{DISCOUNT}_minlr{LEARN_MIN}'\n",
        "\n",
        "class ModifiedTensorBoard(TensorBoard):\n",
        "\n",
        "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.step = 1\n",
        "        self.writer = tf.summary.create_file_writer(self.log_dir)\n",
        "        self._train_dir = self.log_dir + '\\\\train'\n",
        "        self._train_step = 1\n",
        "\n",
        "    # Overriding this method to stop creating default log writer\n",
        "    def set_model(self, model):\n",
        "        pass\n",
        "\n",
        "    # Overrided, saves logs with our step number\n",
        "    # (otherwise every .fit() will start writing from 0th step)\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.update_stats(**logs)\n",
        "\n",
        "    # Overrided\n",
        "    # We train for one batch only, no need to save anything at epoch end\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        pass\n",
        "\n",
        "    # Overrided, so won't close writer\n",
        "    def on_train_end(self, _):\n",
        "        pass\n",
        "\n",
        "    # Custom method for saving own metrics\n",
        "    # Creates writer, writes custom metrics and closes writer\n",
        "    def update_stats(self, **stats):\n",
        "        with self.writer.as_default():\n",
        "            for key, value in stats.items():\n",
        "                tf.summary.scalar(key,value,step=self.step)\n",
        "                self.writer.flush()\n",
        "\n",
        "\n",
        "class DQNAgent(object):\n",
        "    def __init__(self, env, model_name=MODEL_NAME, conv_units=64, dense_units=256):\n",
        "        self.env = env\n",
        "        # Deep Q-learning Parameters\n",
        "        self.discount = DISCOUNT\n",
        "        self.learn_rate = learn_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.model = create_dqn(\n",
        "            self.learn_rate, self.env.bs, self.env.board_size ** 2, conv_units, dense_units)\n",
        "\n",
        "        # target model - this is what we predict against every step\n",
        "        self.target_model = create_dqn(\n",
        "            self.learn_rate, self.env.bs, self.env.board_size ** 2, conv_units, dense_units)\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        self.replay_memory = deque(maxlen=MEM_SIZE)\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "        self.tensorboard = ModifiedTensorBoard(\n",
        "            log_dir=f'logs\\\\{model_name}', profile_batch=0)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        board = state.reshape(1, self.env.board_size ** 2)\n",
        "        # unsolved = [i for i, x in enumerate(board[0]) if x==9]\n",
        "\n",
        "        rand = np.random.random() # random value b/w 0 & 1\n",
        "\n",
        "        if rand < self.epsilon: # random move (explore)\n",
        "            move = self.env.randomAction(state)\n",
        "        else:\n",
        "            moves = self.model.predict(np.reshape(state, (1, self.env.board_size, self.env.board_size, 1)))\n",
        "            moves[board!=9] = np.min(moves) # set already clicked tiles to min value\n",
        "            move = np.argmax(moves)\n",
        "\n",
        "        return move\n",
        "\n",
        "    def update_replay_memory(self, transition):\n",
        "        self.replay_memory.append(transition)\n",
        "\n",
        "    def train(self, done):\n",
        "        if len(self.replay_memory) < MEM_SIZE_MIN:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.replay_memory, BATCH_SIZE)\n",
        "\n",
        "        current_states = np.array([transition[0] for transition in batch])\n",
        "        # print(current_states, current_states[0])\n",
        "        # exit()\n",
        "\n",
        "        current_qs_list = self.model.predict(current_states)\n",
        "\n",
        "        new_current_states = np.array([transition[3] for transition in batch])\n",
        "        future_qs_list = self.target_model.predict(new_current_states)\n",
        "\n",
        "        X,y = [], []\n",
        "\n",
        "        for i, (current_state, action, reward, new_current_state, done) in enumerate(batch):\n",
        "            if not done:\n",
        "                max_future_q = np.max(future_qs_list[i])\n",
        "                new_q = reward + DISCOUNT * max_future_q\n",
        "            else:\n",
        "                new_q = reward\n",
        "\n",
        "            current_qs = current_qs_list[i]\n",
        "            current_qs[action] = new_q\n",
        "\n",
        "            X.append(current_state)\n",
        "            y.append(current_qs)\n",
        "\n",
        "        self.model.fit(np.array(X), np.array(y), batch_size=BATCH_SIZE,\n",
        "                       shuffle=False, verbose=0, callbacks=[self.tensorboard]\\\n",
        "                       if done else None)\n",
        "\n",
        "        # updating to determine if we want to update target_model yet\n",
        "        if done:\n",
        "            self.target_update_counter += 1\n",
        "\n",
        "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
        "            self.target_model.set_weights(self.model.get_weights())\n",
        "            self.target_update_counter = 0\n",
        "\n",
        "        # decay learn_rate\n",
        "        self.learn_rate = max(LEARN_MIN, self.learn_rate*LEARN_DECAY)\n",
        "\n",
        "        # decay epsilon\n",
        "        self.epsilon = max(EPSILON_MIN, self.epsilon*EPSILON_DECAY)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    DQNAgent(MinesweeperDiscreetEnv(board_size=4, num_mines=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270,
          "referenced_widgets": [
            "61a87c5a8a2b44b8b8b4b02c84fb6d9e",
            "b0c6456c89764fcd84c1d48d884e3f87",
            "991d0b5d99454788972415058ed62745",
            "626f79ccc597446c95ab73c2f950f94f",
            "550c06cd7c8944c3b7b9234588b1b8fd",
            "464132f19c014715ab1cd01529325cd7",
            "634f1eba5a6f4df5abfb6cfc00418c44",
            "5719de339ab64340903767e70e01a88e",
            "0294dd0b9b55428699715bd374b348d2",
            "4ae1588d5d664ca78817fb656bf2f1aa",
            "b701f735bd9b48b8a6e79cb511e9f776"
          ]
        },
        "id": "C8F3JwNzEGT4",
        "outputId": "826ad6d8-70c5-4e4b-ff6a-5d3c7b03d8de"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61a87c5a8a2b44b8b8b4b02c84fb6d9e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 300, Mean progress: 0.5832, Mean reward: -0.3362, Win rate : 0.0664, Epsilon: 0.9476042459652978\n",
            "Episode: 400, Mean progress: 0.5908, Mean reward: -0.3052, Win rate : 0.0748, Epsilon: 0.9383160475147614\n",
            "Episode: 500, Mean progress: 0.5802, Mean reward: -0.321, Win rate : 0.0699, Epsilon: 0.9299554837141395\n",
            "Episode: 600, Mean progress: 0.5851, Mean reward: -0.3201, Win rate : 0.0682, Epsilon: 0.9211165713426723\n",
            "Episode: 700, Mean progress: 0.573, Mean reward: -0.3258, Win rate : 0.0685, Epsilon: 0.9130918603446204\n",
            "Episode: 800, Mean progress: 0.5682, Mean reward: -0.3323, Win rate : 0.0649, Epsilon: 0.9046619819597959\n",
            "Episode: 900, Mean progress: 0.5701, Mean reward: -0.333, Win rate : 0.0633, Epsilon: 0.8960858779154546\n",
            "Episode: 1000, Mean progress: 0.5683, Mean reward: -0.3269, Win rate : 0.0639, Epsilon: 0.8873026506365144\n",
            "Episode: 1100, Mean progress: 0.57, Mean reward: -0.3224, Win rate : 0.0627, Epsilon: 0.8782321818557914\n",
            "Episode: 1200, Mean progress: 0.5715, Mean reward: -0.3222, Win rate : 0.0616, Epsilon: 0.8697109202220346\n",
            "Episode: 1300, Mean progress: 0.5719, Mean reward: -0.3281, Win rate : 0.0607, Epsilon: 0.8621124987481024\n",
            "Episode: 1400, Mean progress: 0.5717, Mean reward: -0.3205, Win rate : 0.0628, Epsilon: 0.8535555682542637\n",
            "Episode: 1500, Mean progress: 0.5697, Mean reward: -0.3284, Win rate : 0.06, Epsilon: 0.8459079385001587\n"
          ]
        }
      ],
      "source": [
        "import argparse, pickle\n",
        "from tqdm.notebook import tqdm\n",
        "from keras.models import load_model\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "AGG_STATS_EVERY = 100 # calculate stats every 100 games for tensorboard\n",
        "SAVE_MODEL_EVERY = 10_000 # save model and replay every 10,000 episodes\n",
        "\n",
        "BOARD_SIZE = 4\n",
        "NUM_MINES = 3\n",
        "\n",
        "def main():\n",
        "    env = MinesweeperDiscreetEnv(board_size=BOARD_SIZE, num_mines=NUM_MINES)\n",
        "    agent = DQNAgent(env)\n",
        "\n",
        "    progress_list, wins_list, ep_rewards = [], [], []\n",
        "    n_clicks, n_wins, n_games, score = 0, 0, 0, 0\n",
        "\n",
        "    \n",
        "    for episode in tqdm(range(100000)):\n",
        "        agent.tensorboard.step = episode\n",
        "\n",
        "        current_state = env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            action = agent.get_action(current_state)\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "\n",
        "            agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
        "            agent.train(done)\n",
        "\n",
        "            n_clicks += 1\n",
        "            current_state = new_state\n",
        "\n",
        "            if done: \n",
        "              score += np.count_nonzero((current_state != [9]) & (current_state != [-1])) / (BOARD_SIZE * BOARD_SIZE - NUM_MINES)\n",
        "              if reward == 1: n_wins += 1\n",
        "\n",
        "        # progress_list.append(env.n_progress) # n of non-guess moves\n",
        "        n_games += 1\n",
        "        ep_rewards.append(episode_reward)\n",
        "\n",
        "\n",
        "        if len(agent.replay_memory) < MEM_SIZE_MIN:\n",
        "            continue\n",
        "\n",
        "        if not episode % AGG_STATS_EVERY:\n",
        "            med_progress = round(score / (episode + 1), 4)\n",
        "            win_rate = round(n_wins / n_games, 4)\n",
        "            med_reward = round(sum(ep_rewards) / (episode + 1), 4)\n",
        "\n",
        "            agent.tensorboard.update_stats(\n",
        "                progress_med = med_progress,\n",
        "                winrate = win_rate,\n",
        "                reward_med = med_reward,\n",
        "                learn_rate = agent.learn_rate,\n",
        "                epsilon = agent.epsilon)\n",
        "\n",
        "            print(f'Episode: {episode}, Mean progress: {med_progress}, Mean reward: {med_reward}, Win rate : {win_rate}, Epsilon: {agent.epsilon}')\n",
        "\n",
        "        if not episode % SAVE_MODEL_EVERY:\n",
        "            with open(f'replay/model.pkl', 'wb') as output:\n",
        "                pickle.dump(agent.replay_memory, output)\n",
        "\n",
        "            agent.model.save(f'models/model.h5')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DQL",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0294dd0b9b55428699715bd374b348d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "464132f19c014715ab1cd01529325cd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ae1588d5d664ca78817fb656bf2f1aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "550c06cd7c8944c3b7b9234588b1b8fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b701f735bd9b48b8a6e79cb511e9f776",
            "placeholder": "​",
            "style": "IPY_MODEL_4ae1588d5d664ca78817fb656bf2f1aa",
            "value": " 1508/100000 [11:25&lt;12:30:46,  2.19it/s]"
          }
        },
        "5719de339ab64340903767e70e01a88e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61a87c5a8a2b44b8b8b4b02c84fb6d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_991d0b5d99454788972415058ed62745",
              "IPY_MODEL_626f79ccc597446c95ab73c2f950f94f",
              "IPY_MODEL_550c06cd7c8944c3b7b9234588b1b8fd"
            ],
            "layout": "IPY_MODEL_b0c6456c89764fcd84c1d48d884e3f87"
          }
        },
        "626f79ccc597446c95ab73c2f950f94f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0294dd0b9b55428699715bd374b348d2",
            "max": 100000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5719de339ab64340903767e70e01a88e",
            "value": 1508
          }
        },
        "634f1eba5a6f4df5abfb6cfc00418c44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "991d0b5d99454788972415058ed62745": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_634f1eba5a6f4df5abfb6cfc00418c44",
            "placeholder": "​",
            "style": "IPY_MODEL_464132f19c014715ab1cd01529325cd7",
            "value": "  2%"
          }
        },
        "b0c6456c89764fcd84c1d48d884e3f87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b701f735bd9b48b8a6e79cb511e9f776": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
